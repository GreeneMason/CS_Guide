Each data type has numerous stand methods of encoding information or data into 1s and 0s.

### Definitions
Binary: Base 2, represented by 0 or 1. Each 0 or 1 is a bit.

Hexadecimal: Base 16, represented by 1-9 && A-F.

ASCII: Uses 8 bit bytes with the leading bit ignored.

Unicode: 16, 24, and 32 bits per character.

### Bits
Early computers used 12 or 16 bits for a word size, then it doubled to 32 for a long time and now we run on 64 bits. All of which come back to the computer as 1s and 0s.

